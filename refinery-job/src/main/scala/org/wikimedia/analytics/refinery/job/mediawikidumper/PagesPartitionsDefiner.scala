package org.wikimedia.analytics.refinery.job.mediawikidumper

import org.apache.log4j.Logger
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.functions.{coalesce, col, lit, sum}
import org.apache.spark.sql.{DataFrame, SparkSession}

import scala.math.ceil
import java.util

/**
 * Class to compute define partitions to arrange a dataframe of Page or Revision
 * It will group pages sequentially into partitions not exceeding a certain size.
 * It's computing the partitions at initialization of the instance.
 *
 * @param spark the spark session
 * @param baseRevisionsDF the base revisions dataframe
 * @param pageSizeOverhead the overhead in kB to add to the revision page contents to estimate the page size
 * @param maxTargetFileSize the maximum size of the target XML files in MB
 * @param minChunkSize the minimum size of a chunk of pages in MB. Internally used to pre-group pages before
 *                     regrouping them for final arrangement on the driver. For example, if we have 100 revisions of
 *                     10kB each, for the purpose of determining the partition, the executor will send to the driver a
 *                     1MB chunk of pages.
 * @param internalTargetNumberOfElementsInSparkPartitions Internal number of elements per executor when performing the
 *                                                        partitioning. Used internally for consistency of the output
 *                                                        by controlling the number of Spark partition. See more
 *                                                        comments about it lower.
 */
class PagesPartitionsDefiner(
    spark: SparkSession,
    baseRevisionsDF: DataFrame,
    pageSizeOverhead: Integer,
    maxTargetFileSize: Integer,
    minChunkSize: Integer,
    internalTargetNumberOfElementsInSparkPartitions: Integer = 100000
// Use org.apache.spark.internal.Logging since it is Serializable
) extends Logging with Serializable {

    assert(minChunkSize <= maxTargetFileSize, "minChunkSize must be smaller than maxTargetFileSize")

    val pagesPartitions: List[PagesPartition] = definePartitions

    private def definePartitions: List[PagesPartition] = {
        // At first, before grouping, it's as if each page is its own Partition. So there is a 1-1 relationship between
        // a page and its partition.

        // Step 1: Optimization. on each RDD-partition, we group the pages into page-partitions. The goal of
        // this optimisation is to avoid sending to many elements to the driver in the next step.
        val preMergedPagesPartitions: RDD[PagesPartition] = pagesPartitionsSourceRDD
            .mapPartitions(mergeSequentiallyBySize(_, minChunkSize))

        // Step 2: Retrieve the PagesPartitions from the RDD-partitions into the Spark driver and group them again.
        // This time using the target file size.
        // TODO add comments about the size of the list generated by the collect
        //     At ..... we have x elements in this list for enwiki / wikidata
        val collectedPagesPartitions = preMergedPagesPartitions.collect

        log.info(s"Mediawiki Dumper: Collecting ${collectedPagesPartitions.length} preMerged partitions to the driver")

        val pagesPartitionsWithoutPartitionId: Iterator[PagesPartition] = mergeSequentiallyBySize(
            collectedPagesPartitions.toIterator,
            maxTargetFileSize * 1024 * 1024
        )

        // Step 3: Add the sparkPartitionId to each PagesPartition
        var currentPartitionId: Int = -1  // First partition is 0
        pagesPartitionsWithoutPartitionId.map( pagesPartition => {
            currentPartitionId += 1
            pagesPartition.copy(sparkPartitionId = Some(currentPartitionId))
        }).toList
    }

    import spark.implicits._  // Used when casting the DF to DF[PageBoundary]

    def pagesPartitionsSourceRDD: RDD[PagesPartition] = {
        baseRevisionsDF
            .withColumn("revision_size_with_overhead",
                coalesce(col("size"), lit(0)) + pageSizeOverhead * 1024)
            .groupBy(col("pageId"))
            .agg(sum(col("revision_size_with_overhead")).as("size"))
            .sort(col("pageId").asc)
            .withColumnRenamed("pageId", "startPageId")
            .withColumn("endPageId", col("startPageId"))
            .withColumn("sparkPartitionId", lit(null))
            .as[PagesPartition]
            .rdd
            .coalesce(numberOfSparkPartitionsForCalculation)  // control the number of Spark partitions
    }

    // Optimization: For consistency of the output, we would like to control the number of Spark partition
    // when defining PagesPartitions.
    //
    // Part of the partitioner job is distributed to the executors, but the final grouping is done on the driver.
    // So the internal Spark partitioning could slightly change the output by design.
    //
    // By limiting the number of spark partitions we limit this problem.
    private def numberOfSparkPartitionsForCalculation: Int = {
        ceil(baseRevisionsDF.count.toFloat / internalTargetNumberOfElementsInSparkPartitions.toFloat).toInt
    }

    // Optimization: to determine the partition of a page, it's faster to use an array of pageId boundaries.
    // This is the way Spark RangePartitioner works.
    val partitionsEndPageIds: Array[Long] = pagesPartitions.map(_.endPageId).toArray

    // Optimizations: Memoization of the first and last pageId in the dataset.
    val absoluteStartPageId: Long = pagesPartitions.head.startPageId
    val absoluteEndPageId: Long = partitionsEndPageIds.last

    // Optimization: Wrapper for binary search
    private val binarySearch: (Array[Long], Long) => Int = { (l, x) => util.Arrays.binarySearch(l, x) }

    def getPagesPartition(pageId: Long): PagesPartition = {
        // Sanity checks
        assert(pageId >= absoluteStartPageId, "pageId must be greater than or equal to absoluteStartPageId")
        assert(pageId <= absoluteEndPageId, "pageId must be smaller than or equal to absoluteEndPageId")

        // Optimization ported from Spark RangePartitioner
        // https://github.com/apache/spark/blob/de351e30a90dd988b133b3d00fa6218bfcaba8b8/core/src/main/scala/org/apache/spark/Partitioner.scala#L140
        var partition = 0
        if (partitionsEndPageIds.length <= 128) {
            // If we have less than 128 partitions naive search
            while (partition < partitionsEndPageIds.length && pageId > partitionsEndPageIds(partition)) {
                partition += 1
            }
        } else {
            // Determine which binary search method to use only once.
            partition = binarySearch(partitionsEndPageIds, pageId)
            // binarySearch either returns the match location or -[insertion point]-1
            if (partition < 0) {
                partition = -partition - 1
            }
            if (partition > partitionsEndPageIds.length) {
                partition = partitionsEndPageIds.length
            }
        }
        pagesPartitions(partition)
    }

    /**
     * Method to sequentially create "group of pages" (aka partitions) not exceeding a provided size.
     * It's a local computation.
     *
     * At first, before grouping into partition, it's as if each page is its own partition. So there is a 1-1
     * relationship between a page and its partition. The partition is the page itself.
     *
     * We close the partition each time a merge between partitions is making the result reaching the limit.
     * So each partition size is slightly oversizing the provided max size. It's by design, as we don't have a max
     * size for a single page. Also, as most pages are much smaller than the target file size, the oversizing is
     * negligible.
     *
     * @param pagesPartitions An iterator of PagePartitions containing all metadata about partitions.
     *                   It's expected to be sorted by page id.
     * @param maxPartitionSize the maximum size a partition could not exceed (in MB).
     * @return the pageId boundaries and their sizes.
     */
    def mergeSequentiallyBySize(pagesPartitions: Iterator[PagesPartition], maxPartitionSize: Int): Iterator[PagesPartition] = {
        // The sum of the content size of the pages in the current partition.
        var currentPartitionSize: Long = 0L

        // A variable to hold a result in the code.
        var result: PagesPartition = PagesPartition(0L, 0L, 0L, Some(0))

        // The first page ID in the current partition.
        var startPageId: Option[Long] = None

        // Convert the maxPartitionSize from MB to bytes
        val maxPartitionSizeInBytes = maxPartitionSize * 1024 * 1024

        pagesPartitions.flatMap(pagesPartition => {
            currentPartitionSize += pagesPartition.size  // Add the current page size to the size of the partition

            // At first iteration or when we go to the next partition to merge
            if (startPageId.isEmpty) startPageId = Some(pagesPartition.startPageId)

            if (currentPartitionSize >= maxPartitionSizeInBytes) {
                // Return the boundary if the size reaches the limit.
                result = pagesPartition.sizeAndStartUpdate(currentPartitionSize, startPageId.get)
                // Reset the counters
                startPageId = None
                currentPartitionSize = 0L
                List(result)

            } else if (!pagesPartitions.hasNext) {
                // Reaching end of list, returning the last element, event if the size limit is not reached
                List(pagesPartition.sizeAndStartUpdate(currentPartitionSize, startPageId.get))

            } else {  // Passing / Not merging intermediary partitions
                List()
            }
        })
    }
}
