<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>org.wikimedia.analytics.refinery</groupId>
        <artifactId>refinery</artifactId>
        <version>0.2.61-SNAPSHOT</version>
    </parent>

    <groupId>org.wikimedia.analytics.refinery.spark</groupId>
    <artifactId>refinery-spark</artifactId>
    <packaging>jar</packaging>
    <name>Wikimedia Analytics Refinery Spark</name>

    <dependencies>
        <?SORTPOM IGNORE?>
        <dependency>
            <groupId>org.wikimedia.analytics.refinery.core</groupId>
            <artifactId>refinery-core</artifactId>
        </dependency>
        <dependency>
            <groupId>org.wikimedia.analytics.refinery.tools</groupId>
            <artifactId>refinery-tools</artifactId>
        </dependency>
        <dependency>
            <!--
                 adding explicit dep for snappy and netty otherwise
                 spark assumes is on the java.library.path
                 see: https://github.com/rvs/snappy-java/blob/master/src/main/resources/org/xerial/snappy/SnappyNativeLoader.java#L47
            -->
            <groupId>org.xerial.snappy</groupId>
            <artifactId>snappy-java</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>io.netty</groupId>
            <artifactId>netty-all</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <!-- javax.servlet in hadoop-common is older than the one in spark -->
                    <groupId>javax.servlet</groupId>
                    <artifactId>*</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <!-- Explicitly needed to use DataFrameSuiteBase (tests with SparkSession) -->
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <!-- javax.servlet in hadoop-common is older than the one in spark -->
                    <groupId>com.fasterxml.jackson.core</groupId>
                    <artifactId>*</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <!--
                Explicit dependency before any spark related dependency
                to circumvent version mismatch
            -->
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.compat.version}</artifactId>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <!--
                Order is important here !
                The CompressionCodecName class is defined in multiple
                dependencies and the spark-sql one should be used.
            -->
            <!--
            Force commons-lang3 version to prevent a bug in JSON date parsing in Spark
            -->
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-lang3</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.compat.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.compat.version}</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>com.netaporter</groupId>
            <artifactId>scala-uri_${scala.compat.version}</artifactId>
        </dependency>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.compat.version}</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>com.github.nscala-time</groupId>
            <artifactId>nscala-time_${scala.compat.version}</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-avro_${scala.compat.version}</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-graphx_${scala.compat.version}</artifactId>
        </dependency>
        <dependency>
            <groupId>graphframes</groupId>
            <artifactId>graphframes</artifactId>
        </dependency>
        <dependency>
            <groupId>com.holdenkarau</groupId>
            <artifactId>spark-testing-base_${scala.compat.version}</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
          <groupId>org.yaml</groupId>
          <artifactId>snakeyaml</artifactId>
        </dependency>
        <dependency>
            <groupId>org.scalamock</groupId>
            <artifactId>scalamock-scalatest-support_${scala.compat.version}</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.httpcomponents</groupId>
            <artifactId>httpclient</artifactId>
        </dependency>
        <dependency>
            <!-- Needed to query Hive instead of Spark for ALTER TABLE in Refine job -->
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-jdbc</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <!-- Needed to prevent warnings from hive-jdbc at test time -->
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-annotations</artifactId>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>com.datastax.spark</groupId>
            <artifactId>spark-cassandra-connector_${scala.compat.version}</artifactId>
        </dependency>
        <dependency>
            <groupId>com.amazon.deequ</groupId>
            <artifactId>deequ</artifactId>
        </dependency>
        <?SORTPOM RESUME?>
    </dependencies>

    <build>
        <pluginManagement>
            <plugins>
                <plugin>
                    <!-- Too many violations at the moment -->
                    <!-- TODO: re-enable once violations are fixed -->
                    <groupId>org.basepom.maven</groupId>
                    <artifactId>duplicate-finder-maven-plugin</artifactId>
                    <executions>
                        <execution>
                            <id>duplicate-classes-check</id>
                            <phase>none</phase>
                        </execution>
                    </executions>
                </plugin>
            </plugins>
        </pluginManagement>
        <plugins>
            <plugin>
                <groupId>org.scalatest</groupId>
                <artifactId>scalatest-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
</project>
